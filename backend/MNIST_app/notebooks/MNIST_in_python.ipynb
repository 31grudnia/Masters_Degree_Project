{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ce3d32-9f37-4c85-8fe9-d818a8e52234",
   "metadata": {},
   "source": [
    "<h3>  Imports </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f32fe2ec-3166-4ff0-9d23-130da839b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.cm as cm \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c63630",
   "metadata": {},
   "source": [
    "# Fetching datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a238b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = \"/Users/31_grudnia/Desktop/Python/Playground/Masters_Degree_Project/backend/MNIST_app/MNIST_utils_files/signs_with_labels/\"\n",
    "\n",
    "# Train datasets\n",
    "X_train: np.ndarray = np.load(datasets_path + \"X_train.npy\")\n",
    "Y_train: np.ndarray = np.load(datasets_path + \"Y_train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "393733da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test datasets\n",
    "X_test: np.ndarray = np.load(datasets_path + \"X_test.npy\")\n",
    "Y_test: np.ndarray = np.load(datasets_path + \"Y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45e51f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation datasets\n",
    "X_val: np.ndarray = np.load(datasets_path + \"X_val.npy\")\n",
    "Y_val: np.ndarray = np.load(datasets_path + \"Y_val.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03773529",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_val = X_train / 255., X_test / 255., X_val / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08248829",
   "metadata": {},
   "source": [
    "# One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ae605a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(Y_data: np.ndarray) -> np.ndarray:\n",
    "    \n",
    "    Y_ohe: np.ndarray = Y_data.copy()\n",
    "    categories: np.ndarray = sorted(list(set(Y_ohe)))\n",
    "    one_hot_encoded_data: np.ndarray = []\n",
    "    \n",
    "    for index in range(len(Y_ohe)):\n",
    "        row: np.ndarray = np.zeros(shape=(len(categories), ), dtype=int)\n",
    "        row[Y_ohe[index]] = 1\n",
    "        # print(f\"Element {Y_ohe[index]} - for row {row}\")\n",
    "        one_hot_encoded_data.append(row)\n",
    "    \n",
    "    one_hot_encoded_data = np.array(one_hot_encoded_data)\n",
    "    return one_hot_encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48d60aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_ohe: np.ndarray = one_hot_encode(Y_data=Y_train)\n",
    "Y_test_ohe: np.ndarray = one_hot_encode(Y_data=Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dea232c",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b36e0f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizers:\n",
    "    \n",
    "    @staticmethod\n",
    "    def SGD(w, dw, alpha=0.01, beta=0.09):\n",
    "        w = w - alpha * dw\n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ec4b8",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57c463ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunctions:\n",
    "    \n",
    "    @staticmethod\n",
    "    def Categorical_Crossentropy(y_pred, y_true):\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdd3116",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1686768",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activations:\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        exp_x = np.exp(x - np.max(x))  # Subtracting the maximum value for numerical stability\n",
    "        return exp_x / exp_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b7aa1e",
   "metadata": {},
   "source": [
    "# Layers\n",
    "- Dense\n",
    "    - Init of parameters (bias and weights)\n",
    "    - Forward Propagation\n",
    "    - Backward Propagation\n",
    "    - Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97589cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layers:\n",
    "    class Dense:\n",
    "        \n",
    "        def __init__(self, neurons=0, activation=Activations.relu, inputs=0, dropout_rate=1):\n",
    "            # Initialization of weights and biases\n",
    "            self.weights = np.random.randn(neurons, inputs)\n",
    "            self.biases = np.random.randn(1, neurons)\n",
    "            self.activation = activation\n",
    "            self.dropout_rate = dropout_rate\n",
    "        \n",
    "        def forward(self, inputs):\n",
    "            self.inputs = inputs\n",
    "            self.outputs = np.dot(inputs, self.weights.T) + self.biases\n",
    "            self.outputs = self.activation(self.outputs)\n",
    "            self.outputs = self.dropout(self.outputs)\n",
    "            return self.outputs\n",
    "        \n",
    "        def backward(self, error, learning_rate):\n",
    "            self.error = error\n",
    "            self.delta = self.error * self.activation(self.outputs, derivative=True)\n",
    "            self.delta = self.dropout(self.delta, derivative=True)\n",
    "            self.weights -= learning_rate * np.dot(self.delta, self.inputs.T)\n",
    "            self.biases -= learning_rate * np.sum(self.delta, axis=0, keepdims=True)\n",
    "            return self.delta\n",
    "        \n",
    "        def dropout(self, x, derivative=False):\n",
    "            if derivative:\n",
    "                return self.dropout_rate * (1 - self.dropout_rate) * x\n",
    "            return self.dropout_rate * x \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c941644",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71d2c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, layers, loss=LossFunctions.Categorical_Crossentropy, optimizer=Optimizers.SGD, learning_rate=0.01, momentum=0.0, beta1=0.9, beta2=0.999, epsilon=1e-7):\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.velocities = []\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.optimizer_kwargs = {'alpha': self.learning_rate, 'beta1': self.beta1, 'beta2': self.beta2}\n",
    "        if self.optimizer == Optimizers.SGD:\n",
    "            self.optimizer_kwargs['momentum'] = self.momentum\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = self.inputs\n",
    "        for layer in self.layers:\n",
    "            self.output = layer.forward(self.outputs)\n",
    "        return self.outputs\n",
    "    \n",
    "    def backwards(self, targets):\n",
    "        self.targets = targets\n",
    "        self.error = self.loss(self.outputs, self.targets)\n",
    "        self.delta = self.error\n",
    "        for layer in reversed(self.layers):\n",
    "            self.delta = layer.backward(self.delta, self.optimizer_kwargs)\n",
    "        return self.delta\n",
    "        \n",
    "    def update_weights(self):\n",
    "        for layer in self.layers:\n",
    "            layer.update_weights(self.optimizer_kwargs)\n",
    "            \n",
    "    def train(self, inputs, targets, epochs=1, batch_size=1, verbose=False):\n",
    "        self.epochs = epochs\n",
    "        self.epoch_errors = []\n",
    "        self.epoch_losses = []\n",
    "        self.epoch_accuracies = []\n",
    "        self.epoch_times = []\n",
    "        start = time.time()\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_start = time.time()\n",
    "            epoch_error = 0\n",
    "            epoch_loss = 0\n",
    "            epoch_accuracy = 0\n",
    "            for i in range(0, inputs.shape[0], batch_size):\n",
    "                batch_inputs = inputs[i: i+batch_size]\n",
    "                batch_targets = targets[i: i+batch_size]\n",
    "                self.forward(batch_inputs)\n",
    "                self.backward(batch_targets)\n",
    "                self.update_weights()\n",
    "                epoch_error += self.error.sum()\n",
    "                epoch_loss += self.loss(self.outputs, self.targets).sum()\n",
    "                epoch_accuracy += self.accuracy(self.outputs, self.targets)\n",
    "            epoch_time = time.time() - epoch_start\n",
    "            self.epoch_errors.append(epoch_error)\n",
    "            self.epoch_losses.append(epoch_loss)\n",
    "            self.epoch_accuracies.append(epoch_accuracy)\n",
    "            self.epoch_times.append(epoch_time)\n",
    "            if verbose:\n",
    "                print('Epoch: {}, Error: {}, Loss: {}, Accuracy: {}, Time: {}'.format(epoch, epoch_error, epoch_loss, epoch_accuracy, epoch_time))\n",
    "        self.train_time = time.time() - start\n",
    "        return self.epoch_errors, self.epoch_losses, self.epoch_accuracies, self.epoch_times\n",
    "\n",
    "    def accuracy(self, outputs, targets):\n",
    "        return np.sum(np.argmax(outputs, axis=1) == np.argmax(targets, axis=1)) / outputs.shape[0]\n",
    "\n",
    "    def loss(self, outputs, targets):\n",
    "        return self.loss.forward(outputs, targets)\n",
    "        \n",
    "    def predict(self, inputs):\n",
    "        return np.argmax(self.forward(inputs), axis=1)\n",
    "        \n",
    "    def evaluate(self, inputs, targets):\n",
    "        return self.loss(self.forward(inputs), targets), self.accuracy(self.forward(inputs), targets) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6042317f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (10,1024) and (0,128) not aligned: 1024 (dim 1) != 0 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m NeuralNetwork([\n\u001b[1;32m      6\u001b[0m     Layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1028\u001b[39m, Activations\u001b[38;5;241m.\u001b[39mrelu, inputs\u001b[38;5;241m=\u001b[39mX_train_flattened\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]),\n\u001b[1;32m      7\u001b[0m     Layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m128\u001b[39m, Activations\u001b[38;5;241m.\u001b[39mrelu),\n\u001b[1;32m      8\u001b[0m     Layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m89\u001b[39m, Activations\u001b[38;5;241m.\u001b[39msoftmax)\n\u001b[1;32m      9\u001b[0m ], LossFunctions\u001b[38;5;241m.\u001b[39mCategorical_Crossentropy, Optimizers\u001b[38;5;241m.\u001b[39mSGD, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_flattened\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train_ohe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mevaluate(X_test\u001b[38;5;241m.\u001b[39mreshape(X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), Y_test_ohe)\n",
      "Cell \u001b[0;32mIn[35], line 53\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, inputs, targets, epochs, batch_size, verbose)\u001b[0m\n\u001b[1;32m     51\u001b[0m batch_inputs \u001b[38;5;241m=\u001b[39m inputs[i: i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m     52\u001b[0m batch_targets \u001b[38;5;241m=\u001b[39m targets[i: i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward(batch_targets)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_weights()\n",
      "Cell \u001b[0;32mIn[35], line 23\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs\n",
      "Cell \u001b[0;32mIn[34], line 13\u001b[0m, in \u001b[0;36mLayers.Dense.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs \u001b[38;5;241m=\u001b[39m inputs\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs)\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (10,1024) and (0,128) not aligned: 1024 (dim 1) != 0 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Flatten the input data\n",
    "X_train_flattened = X_train.reshape(X_train.shape[0], -1)\n",
    "\n",
    "# Define and build the model\n",
    "model = NeuralNetwork([\n",
    "    Layers.Dense(1028, Activations.relu, inputs=X_train_flattened.shape[1]),\n",
    "    Layers.Dense(128, Activations.relu),\n",
    "    Layers.Dense(89, Activations.softmax)\n",
    "], LossFunctions.Categorical_Crossentropy, Optimizers.SGD, learning_rate=0.01)\n",
    "\n",
    "# Train the model\n",
    "model.train(X_train_flattened, Y_train_ohe, epochs=100, batch_size=10, verbose=True)\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(X_test.reshape(X_test.shape[0], -1), Y_test_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b0b2d",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348b0ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(446524, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_flattened.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dfb6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain shape: (446524, 32, 32)\n",
      "Ytrain shape: (446524,)\n",
      "Ytrainohe shape: 446524\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Xtrain shape: {X_train.shape}\")\n",
    "print(f\"Ytrain shape: {Y_train.shape}\")\n",
    "print(f\"Ytrainohe shape: {len(Y_train_ohe)}\")\n",
    "print(Y_train_ohe[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca728f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(446524, 32, 32) (446524,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61060b0-478a-496a-9bb7-6da28e21c4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(446524,)\n",
      "(446524, 32, 32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x408c83350>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(Y_train.shape)\n",
    "print(X_train.shape)\n",
    "index = 7\n",
    "plt.title((Y_train[index]))\n",
    "plt.imshow(X_train[index].reshape(32, 32), cmap=cm.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6a5dee9-d6a4-4e98-bfdf-d8fbc63515b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/31_grudnia/Desktop/Python/Playground/Masters_Degree_Project/backend/venv/lib/python3.11/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m4466/4466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 8ms/step - accuracy: 0.1843 - loss: 3.4712\n",
      "Epoch 2/15\n",
      "\u001b[1m4466/4466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.5182 - loss: 1.7060\n",
      "Epoch 3/15\n",
      "\u001b[1m4466/4466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 8ms/step - accuracy: 0.5816 - loss: 1.4429\n",
      "Epoch 4/15\n",
      "\u001b[1m4466/4466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 8ms/step - accuracy: 0.6151 - loss: 1.2933\n",
      "Epoch 5/15\n",
      "\u001b[1m4466/4466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 8ms/step - accuracy: 0.6437 - loss: 1.1859\n",
      "Epoch 6/15\n",
      "\u001b[1m4466/4466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 8ms/step - accuracy: 0.6607 - loss: 1.1101\n",
      "Epoch 7/15\n",
      "\u001b[1m4466/4466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.6747 - loss: 1.0541\n",
      "Epoch 8/15\n",
      "\u001b[1m4466/4466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 10ms/step - accuracy: 0.6853 - loss: 1.0076\n",
      "Epoch 9/15\n",
      "\u001b[1m4466/4466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 9ms/step - accuracy: 0.6953 - loss: 0.9689\n",
      "Epoch 10/15\n",
      "\u001b[1m4466/4466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 9ms/step - accuracy: 0.7030 - loss: 0.9389\n",
      "Epoch 11/15\n",
      "\u001b[1m4466/4466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 9ms/step - accuracy: 0.7099 - loss: 0.9089\n",
      "Epoch 12/15\n",
      "\u001b[1m4466/4466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 10ms/step - accuracy: 0.7151 - loss: 0.8896\n",
      "Epoch 13/15\n",
      "\u001b[1m4466/4466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 8ms/step - accuracy: 0.7213 - loss: 0.8656\n",
      "Epoch 14/15\n",
      "\u001b[1m4466/4466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 8ms/step - accuracy: 0.7255 - loss: 0.8476\n",
      "Epoch 15/15\n",
      "\u001b[1m4466/4466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 8ms/step - accuracy: 0.7288 - loss: 0.8289\n",
      "\u001b[1m1745/1745\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7615 - loss: 0.7190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7159880995750427, 0.7611215710639954]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(32, 32)),\n",
    "  tf.keras.layers.Dense(1024, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(89, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train_ohe, epochs=15, batch_size=100)\n",
    "model.evaluate(X_test, Y_test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89204614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1745/1745\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_val)\n",
    "predicted_class_indices = np.argmax(predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44860a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0; Prediction: 79; True Label: 79\n",
      "Index: 1; Prediction: 84; True Label: 84\n",
      "Index: 2; Prediction: 38; True Label: 12\n",
      "Index: 3; Prediction: 11; True Label: 11\n",
      "Index: 4; Prediction: 36; True Label: 36\n",
      "Index: 5; Prediction: 61; True Label: 2\n",
      "Index: 6; Prediction: 26; True Label: 26\n",
      "Index: 7; Prediction: 44; True Label: 44\n",
      "Index: 8; Prediction: 11; True Label: 11\n",
      "Index: 9; Prediction: 49; True Label: 49\n",
      "Index: 10; Prediction: 68; True Label: 68\n",
      "Index: 11; Prediction: 54; True Label: 28\n",
      "Index: 12; Prediction: 43; True Label: 43\n",
      "Index: 13; Prediction: 15; True Label: 15\n",
      "Index: 14; Prediction: 14; True Label: 14\n",
      "Index: 15; Prediction: 44; True Label: 1\n",
      "Index: 16; Prediction: 68; True Label: 68\n",
      "Index: 17; Prediction: 3; True Label: 9\n",
      "Index: 18; Prediction: 59; True Label: 59\n",
      "Index: 19; Prediction: 86; True Label: 86\n",
      "Index: 20; Prediction: 13; True Label: 13\n",
      "Index: 21; Prediction: 48; True Label: 48\n",
      "Index: 22; Prediction: 78; True Label: 78\n",
      "Index: 23; Prediction: 76; True Label: 76\n",
      "Index: 24; Prediction: 71; True Label: 71\n",
      "Index: 25; Prediction: 38; True Label: 38\n",
      "Index: 26; Prediction: 14; True Label: 67\n",
      "Index: 27; Prediction: 27; True Label: 44\n",
      "Index: 28; Prediction: 37; True Label: 37\n",
      "Index: 29; Prediction: 0; True Label: 50\n",
      "Index: 30; Prediction: 5; True Label: 5\n",
      "Index: 31; Prediction: 60; True Label: 27\n",
      "Index: 32; Prediction: 48; True Label: 48\n",
      "Index: 33; Prediction: 39; True Label: 39\n",
      "Index: 34; Prediction: 30; True Label: 56\n",
      "Index: 35; Prediction: 43; True Label: 43\n",
      "Index: 36; Prediction: 4; True Label: 4\n",
      "Index: 37; Prediction: 46; True Label: 46\n",
      "Index: 38; Prediction: 41; True Label: 41\n",
      "Index: 39; Prediction: 69; True Label: 69\n",
      "Index: 40; Prediction: 19; True Label: 19\n",
      "Index: 41; Prediction: 17; True Label: 17\n",
      "Index: 42; Prediction: 62; True Label: 62\n",
      "Index: 43; Prediction: 79; True Label: 70\n",
      "Index: 44; Prediction: 86; True Label: 86\n",
      "Index: 45; Prediction: 68; True Label: 68\n",
      "Index: 46; Prediction: 60; True Label: 60\n",
      "Index: 47; Prediction: 50; True Label: 0\n",
      "Index: 48; Prediction: 36; True Label: 36\n",
      "Index: 49; Prediction: 68; True Label: 68\n",
      "Index: 50; Prediction: 8; True Label: 8\n",
      "Index: 51; Prediction: 49; True Label: 49\n",
      "Index: 52; Prediction: 84; True Label: 84\n",
      "Index: 53; Prediction: 78; True Label: 78\n",
      "Index: 54; Prediction: 31; True Label: 57\n",
      "Index: 55; Prediction: 3; True Label: 3\n",
      "Index: 56; Prediction: 71; True Label: 69\n",
      "Index: 57; Prediction: 86; True Label: 86\n",
      "Index: 58; Prediction: 43; True Label: 43\n",
      "Index: 59; Prediction: 44; True Label: 44\n",
      "Index: 60; Prediction: 11; True Label: 11\n",
      "Index: 61; Prediction: 81; True Label: 81\n",
      "Index: 62; Prediction: 77; True Label: 77\n",
      "Index: 63; Prediction: 26; True Label: 26\n",
      "Index: 64; Prediction: 24; True Label: 24\n",
      "Index: 65; Prediction: 35; True Label: 61\n",
      "Index: 66; Prediction: 9; True Label: 9\n",
      "Index: 67; Prediction: 59; True Label: 16\n",
      "Index: 68; Prediction: 39; True Label: 37\n",
      "Index: 69; Prediction: 17; True Label: 22\n",
      "Index: 70; Prediction: 7; True Label: 18\n",
      "Index: 71; Prediction: 60; True Label: 60\n",
      "Index: 72; Prediction: 61; True Label: 61\n",
      "Index: 73; Prediction: 26; True Label: 26\n",
      "Index: 74; Prediction: 13; True Label: 13\n",
      "Index: 75; Prediction: 72; True Label: 72\n",
      "Index: 76; Prediction: 51; True Label: 51\n",
      "Index: 77; Prediction: 1; True Label: 1\n",
      "Index: 78; Prediction: 72; True Label: 63\n",
      "Index: 79; Prediction: 27; True Label: 27\n",
      "Index: 80; Prediction: 83; True Label: 83\n",
      "Index: 81; Prediction: 3; True Label: 3\n",
      "Index: 82; Prediction: 35; True Label: 35\n",
      "Index: 83; Prediction: 48; True Label: 30\n",
      "Index: 84; Prediction: 21; True Label: 44\n",
      "Index: 85; Prediction: 75; True Label: 75\n",
      "Index: 86; Prediction: 1; True Label: 1\n",
      "Index: 87; Prediction: 31; True Label: 31\n",
      "Index: 88; Prediction: 64; True Label: 64\n",
      "Index: 89; Prediction: 8; True Label: 8\n",
      "Index: 90; Prediction: 56; True Label: 56\n",
      "Index: 91; Prediction: 13; True Label: 13\n",
      "Index: 92; Prediction: 6; True Label: 6\n",
      "Index: 93; Prediction: 30; True Label: 30\n",
      "Index: 94; Prediction: 1; True Label: 1\n",
      "Index: 95; Prediction: 40; True Label: 40\n",
      "Index: 96; Prediction: 32; True Label: 32\n",
      "Index: 97; Prediction: 39; True Label: 39\n",
      "Index: 98; Prediction: 80; True Label: 15\n",
      "Index: 99; Prediction: 61; True Label: 61\n",
      "Index: 100; Prediction: 13; True Label: 13\n",
      "Index: 101; Prediction: 10; True Label: 10\n",
      "Index: 102; Prediction: 68; True Label: 68\n",
      "Index: 103; Prediction: 5; True Label: 5\n",
      "Index: 104; Prediction: 47; True Label: 61\n",
      "Index: 105; Prediction: 6; True Label: 6\n",
      "Index: 106; Prediction: 21; True Label: 21\n",
      "Index: 107; Prediction: 53; True Label: 53\n",
      "Index: 108; Prediction: 34; True Label: 34\n",
      "Index: 109; Prediction: 37; True Label: 37\n",
      "Index: 110; Prediction: 61; True Label: 61\n",
      "Index: 111; Prediction: 36; True Label: 36\n",
      "Index: 112; Prediction: 10; True Label: 22\n",
      "Index: 113; Prediction: 66; True Label: 66\n",
      "Index: 114; Prediction: 85; True Label: 85\n",
      "Index: 115; Prediction: 47; True Label: 47\n",
      "Index: 116; Prediction: 42; True Label: 42\n",
      "Index: 117; Prediction: 0; True Label: 0\n",
      "Index: 118; Prediction: 15; True Label: 15\n",
      "Index: 119; Prediction: 43; True Label: 75\n",
      "Index: 120; Prediction: 10; True Label: 10\n",
      "Index: 121; Prediction: 54; True Label: 54\n",
      "Index: 122; Prediction: 20; True Label: 20\n",
      "Index: 123; Prediction: 75; True Label: 75\n",
      "Index: 124; Prediction: 67; True Label: 76\n",
      "Index: 125; Prediction: 62; True Label: 62\n",
      "Index: 126; Prediction: 68; True Label: 77\n",
      "Index: 127; Prediction: 79; True Label: 70\n",
      "Index: 128; Prediction: 61; True Label: 2\n",
      "Index: 129; Prediction: 38; True Label: 12\n",
      "Index: 130; Prediction: 49; True Label: 49\n",
      "Index: 131; Prediction: 77; True Label: 5\n",
      "Index: 132; Prediction: 66; True Label: 66\n",
      "Index: 133; Prediction: 34; True Label: 34\n",
      "Index: 134; Prediction: 87; True Label: 87\n",
      "Index: 135; Prediction: 31; True Label: 57\n",
      "Index: 136; Prediction: 14; True Label: 14\n",
      "Index: 137; Prediction: 63; True Label: 63\n",
      "Index: 138; Prediction: 52; True Label: 52\n",
      "Index: 139; Prediction: 60; True Label: 60\n",
      "Index: 140; Prediction: 24; True Label: 24\n",
      "Index: 141; Prediction: 20; True Label: 20\n",
      "Index: 142; Prediction: 3; True Label: 3\n",
      "Index: 143; Prediction: 63; True Label: 72\n",
      "Index: 144; Prediction: 57; True Label: 58\n",
      "Index: 145; Prediction: 53; True Label: 53\n",
      "Index: 146; Prediction: 80; True Label: 80\n",
      "Index: 147; Prediction: 50; True Label: 50\n",
      "Index: 148; Prediction: 65; True Label: 65\n",
      "Index: 149; Prediction: 9; True Label: 16\n",
      "Index: 150; Prediction: 67; True Label: 67\n",
      "Index: 151; Prediction: 70; True Label: 70\n",
      "Index: 152; Prediction: 83; True Label: 83\n",
      "Index: 153; Prediction: 11; True Label: 11\n",
      "Index: 154; Prediction: 80; True Label: 80\n",
      "Index: 155; Prediction: 58; True Label: 58\n",
      "Index: 156; Prediction: 24; True Label: 24\n",
      "Index: 157; Prediction: 77; True Label: 67\n",
      "Index: 158; Prediction: 18; True Label: 18\n",
      "Index: 159; Prediction: 40; True Label: 40\n",
      "Index: 160; Prediction: 71; True Label: 71\n",
      "Index: 161; Prediction: 25; True Label: 25\n",
      "Index: 162; Prediction: 26; True Label: 26\n",
      "Index: 163; Prediction: 3; True Label: 3\n",
      "Index: 164; Prediction: 0; True Label: 50\n",
      "Index: 165; Prediction: 69; True Label: 78\n",
      "Index: 166; Prediction: 39; True Label: 39\n",
      "Index: 167; Prediction: 6; True Label: 6\n",
      "Index: 168; Prediction: 56; True Label: 56\n",
      "Index: 169; Prediction: 69; True Label: 70\n",
      "Index: 170; Prediction: 63; True Label: 68\n",
      "Index: 171; Prediction: 9; True Label: 62\n",
      "Index: 172; Prediction: 61; True Label: 2\n",
      "Index: 173; Prediction: 88; True Label: 88\n",
      "Index: 174; Prediction: 56; True Label: 56\n",
      "Index: 175; Prediction: 4; True Label: 4\n",
      "Index: 176; Prediction: 12; True Label: 12\n",
      "Index: 177; Prediction: 62; True Label: 62\n",
      "Index: 178; Prediction: 55; True Label: 55\n",
      "Index: 179; Prediction: 38; True Label: 12\n",
      "Index: 180; Prediction: 50; True Label: 0\n",
      "Index: 181; Prediction: 23; True Label: 23\n",
      "Index: 182; Prediction: 31; True Label: 31\n",
      "Index: 183; Prediction: 21; True Label: 21\n",
      "Index: 184; Prediction: 27; True Label: 27\n",
      "Index: 185; Prediction: 18; True Label: 18\n",
      "Index: 186; Prediction: 76; True Label: 76\n",
      "Index: 187; Prediction: 82; True Label: 82\n",
      "Index: 188; Prediction: 63; True Label: 63\n",
      "Index: 189; Prediction: 61; True Label: 61\n",
      "Index: 190; Prediction: 58; True Label: 32\n",
      "Index: 191; Prediction: 5; True Label: 5\n",
      "Index: 192; Prediction: 71; True Label: 71\n",
      "Index: 193; Prediction: 76; True Label: 67\n",
      "Index: 194; Prediction: 39; True Label: 39\n",
      "Index: 195; Prediction: 37; True Label: 37\n",
      "Index: 196; Prediction: 66; True Label: 66\n",
      "Index: 197; Prediction: 26; True Label: 26\n",
      "Index: 198; Prediction: 63; True Label: 63\n",
      "Index: 199; Prediction: 38; True Label: 38\n",
      "Index: 200; Prediction: 7; True Label: 7\n",
      "Index: 201; Prediction: 38; True Label: 38\n",
      "Index: 202; Prediction: 71; True Label: 71\n",
      "Index: 203; Prediction: 55; True Label: 55\n",
      "Index: 204; Prediction: 45; True Label: 45\n",
      "Index: 205; Prediction: 38; True Label: 38\n",
      "Index: 206; Prediction: 80; True Label: 80\n",
      "Index: 207; Prediction: 32; True Label: 32\n",
      "Index: 208; Prediction: 16; True Label: 9\n",
      "Index: 209; Prediction: 87; True Label: 87\n",
      "Index: 210; Prediction: 59; True Label: 33\n",
      "Index: 211; Prediction: 40; True Label: 77\n",
      "Index: 212; Prediction: 28; True Label: 28\n",
      "Index: 213; Prediction: 71; True Label: 71\n",
      "Index: 214; Prediction: 40; True Label: 40\n",
      "Index: 215; Prediction: 22; True Label: 23\n",
      "Index: 216; Prediction: 79; True Label: 70\n",
      "Index: 217; Prediction: 22; True Label: 48\n",
      "Index: 218; Prediction: 69; True Label: 69\n",
      "Index: 219; Prediction: 11; True Label: 11\n",
      "Index: 220; Prediction: 49; True Label: 49\n",
      "Index: 221; Prediction: 5; True Label: 5\n",
      "Index: 222; Prediction: 85; True Label: 85\n",
      "Index: 223; Prediction: 40; True Label: 40\n",
      "Index: 224; Prediction: 43; True Label: 43\n",
      "Index: 225; Prediction: 24; True Label: 24\n",
      "Index: 226; Prediction: 40; True Label: 40\n",
      "Index: 227; Prediction: 39; True Label: 39\n",
      "Index: 228; Prediction: 46; True Label: 46\n",
      "Index: 229; Prediction: 24; True Label: 24\n",
      "Index: 230; Prediction: 79; True Label: 79\n",
      "Index: 231; Prediction: 58; True Label: 58\n",
      "Index: 232; Prediction: 55; True Label: 55\n",
      "Index: 233; Prediction: 85; True Label: 85\n",
      "Index: 234; Prediction: 27; True Label: 49\n",
      "Index: 235; Prediction: 83; True Label: 83\n",
      "Index: 236; Prediction: 53; True Label: 53\n",
      "Index: 237; Prediction: 22; True Label: 22\n",
      "Index: 238; Prediction: 36; True Label: 36\n",
      "Index: 239; Prediction: 55; True Label: 55\n",
      "Index: 240; Prediction: 88; True Label: 88\n",
      "Index: 241; Prediction: 86; True Label: 86\n",
      "Index: 242; Prediction: 54; True Label: 28\n",
      "Index: 243; Prediction: 31; True Label: 31\n",
      "Index: 244; Prediction: 10; True Label: 10\n",
      "Index: 245; Prediction: 10; True Label: 10\n",
      "Index: 246; Prediction: 50; True Label: 52\n",
      "Index: 247; Prediction: 47; True Label: 47\n",
      "Index: 248; Prediction: 41; True Label: 41\n",
      "Index: 249; Prediction: 74; True Label: 74\n",
      "Index: 250; Prediction: 32; True Label: 32\n",
      "Index: 251; Prediction: 72; True Label: 63\n",
      "Index: 252; Prediction: 74; True Label: 74\n",
      "Index: 253; Prediction: 20; True Label: 20\n",
      "Index: 254; Prediction: 6; True Label: 11\n",
      "Index: 255; Prediction: 15; True Label: 15\n",
      "Index: 256; Prediction: 88; True Label: 88\n",
      "Index: 257; Prediction: 55; True Label: 55\n",
      "Index: 258; Prediction: 20; True Label: 20\n",
      "Index: 259; Prediction: 23; True Label: 23\n",
      "Index: 260; Prediction: 57; True Label: 31\n",
      "Index: 261; Prediction: 79; True Label: 70\n",
      "Index: 262; Prediction: 27; True Label: 31\n",
      "Index: 263; Prediction: 65; True Label: 65\n",
      "Index: 264; Prediction: 43; True Label: 43\n",
      "Index: 265; Prediction: 29; True Label: 29\n",
      "Index: 266; Prediction: 39; True Label: 39\n",
      "Index: 267; Prediction: 10; True Label: 10\n",
      "Index: 268; Prediction: 31; True Label: 31\n",
      "Index: 269; Prediction: 30; True Label: 30\n",
      "Index: 270; Prediction: 44; True Label: 44\n",
      "Index: 271; Prediction: 85; True Label: 85\n",
      "Index: 272; Prediction: 62; True Label: 62\n",
      "Index: 273; Prediction: 60; True Label: 60\n",
      "Index: 274; Prediction: 21; True Label: 21\n",
      "Index: 275; Prediction: 65; True Label: 2\n",
      "Index: 276; Prediction: 17; True Label: 17\n",
      "Index: 277; Prediction: 56; True Label: 56\n",
      "Index: 278; Prediction: 4; True Label: 4\n",
      "Index: 279; Prediction: 85; True Label: 85\n",
      "Index: 280; Prediction: 62; True Label: 62\n",
      "Index: 281; Prediction: 59; True Label: 59\n",
      "Index: 282; Prediction: 32; True Label: 32\n",
      "Index: 283; Prediction: 56; True Label: 57\n",
      "Index: 284; Prediction: 85; True Label: 85\n",
      "Index: 285; Prediction: 48; True Label: 48\n",
      "Index: 286; Prediction: 1; True Label: 1\n",
      "Index: 287; Prediction: 47; True Label: 47\n",
      "Index: 288; Prediction: 61; True Label: 61\n",
      "Index: 289; Prediction: 58; True Label: 58\n",
      "Index: 290; Prediction: 27; True Label: 1\n",
      "Index: 291; Prediction: 10; True Label: 53\n",
      "Index: 292; Prediction: 5; True Label: 5\n",
      "Index: 293; Prediction: 70; True Label: 74\n",
      "Index: 294; Prediction: 83; True Label: 83\n",
      "Index: 295; Prediction: 49; True Label: 49\n",
      "Index: 296; Prediction: 70; True Label: 70\n",
      "Index: 297; Prediction: 62; True Label: 62\n",
      "Index: 298; Prediction: 0; True Label: 0\n",
      "Index: 299; Prediction: 22; True Label: 23\n",
      "Index: 300; Prediction: 48; True Label: 48\n",
      "Index: 301; Prediction: 79; True Label: 19\n",
      "Index: 302; Prediction: 52; True Label: 52\n",
      "Index: 303; Prediction: 27; True Label: 31\n",
      "Index: 304; Prediction: 42; True Label: 42\n",
      "Index: 305; Prediction: 9; True Label: 9\n",
      "Index: 306; Prediction: 53; True Label: 78\n",
      "Index: 307; Prediction: 86; True Label: 86\n",
      "Index: 308; Prediction: 83; True Label: 44\n",
      "Index: 309; Prediction: 76; True Label: 76\n",
      "Index: 310; Prediction: 24; True Label: 0\n",
      "Index: 311; Prediction: 41; True Label: 41\n",
      "Index: 312; Prediction: 45; True Label: 45\n",
      "Index: 313; Prediction: 64; True Label: 64\n",
      "Index: 314; Prediction: 79; True Label: 79\n",
      "Index: 315; Prediction: 4; True Label: 4\n",
      "Index: 316; Prediction: 77; True Label: 68\n",
      "Index: 317; Prediction: 35; True Label: 2\n",
      "Index: 318; Prediction: 84; True Label: 84\n",
      "Index: 319; Prediction: 84; True Label: 84\n",
      "Index: 320; Prediction: 64; True Label: 64\n",
      "Index: 321; Prediction: 43; True Label: 43\n",
      "Index: 322; Prediction: 47; True Label: 47\n",
      "Index: 323; Prediction: 23; True Label: 23\n",
      "Index: 324; Prediction: 31; True Label: 31\n",
      "Index: 325; Prediction: 16; True Label: 16\n",
      "Index: 326; Prediction: 50; True Label: 50\n",
      "Index: 327; Prediction: 26; True Label: 26\n",
      "Index: 328; Prediction: 16; True Label: 16\n",
      "Index: 329; Prediction: 88; True Label: 88\n",
      "Index: 330; Prediction: 0; True Label: 24\n",
      "Index: 331; Prediction: 9; True Label: 9\n",
      "Index: 332; Prediction: 83; True Label: 68\n",
      "Index: 333; Prediction: 7; True Label: 7\n",
      "Index: 334; Prediction: 56; True Label: 56\n",
      "Index: 335; Prediction: 5; True Label: 5\n",
      "Index: 336; Prediction: 3; True Label: 3\n",
      "Index: 337; Prediction: 32; True Label: 58\n",
      "Index: 338; Prediction: 68; True Label: 68\n",
      "Index: 339; Prediction: 48; True Label: 22\n",
      "Index: 340; Prediction: 5; True Label: 5\n",
      "Index: 341; Prediction: 63; True Label: 63\n",
      "Index: 342; Prediction: 53; True Label: 53\n",
      "Index: 343; Prediction: 30; True Label: 30\n",
      "Index: 344; Prediction: 40; True Label: 40\n",
      "Index: 345; Prediction: 34; True Label: 42\n",
      "Index: 346; Prediction: 43; True Label: 43\n",
      "Index: 347; Prediction: 62; True Label: 26\n",
      "Index: 348; Prediction: 3; True Label: 3\n",
      "Index: 349; Prediction: 0; True Label: 0\n",
      "Index: 350; Prediction: 70; True Label: 70\n",
      "Index: 351; Prediction: 35; True Label: 35\n",
      "Index: 352; Prediction: 82; True Label: 82\n",
      "Index: 353; Prediction: 64; True Label: 64\n",
      "Index: 354; Prediction: 60; True Label: 60\n",
      "Index: 355; Prediction: 8; True Label: 8\n",
      "Index: 356; Prediction: 80; True Label: 80\n",
      "Index: 357; Prediction: 41; True Label: 41\n",
      "Index: 358; Prediction: 15; True Label: 15\n",
      "Index: 359; Prediction: 10; True Label: 10\n",
      "Index: 360; Prediction: 2; True Label: 2\n",
      "Index: 361; Prediction: 85; True Label: 85\n",
      "Index: 362; Prediction: 58; True Label: 32\n",
      "Index: 363; Prediction: 39; True Label: 39\n",
      "Index: 364; Prediction: 17; True Label: 11\n",
      "Index: 365; Prediction: 67; True Label: 67\n",
      "Index: 366; Prediction: 84; True Label: 27\n",
      "Index: 367; Prediction: 27; True Label: 27\n",
      "Index: 368; Prediction: 85; True Label: 85\n",
      "Index: 369; Prediction: 69; True Label: 69\n",
      "Index: 370; Prediction: 84; True Label: 84\n",
      "Index: 371; Prediction: 44; True Label: 44\n",
      "Index: 372; Prediction: 26; True Label: 26\n",
      "Index: 373; Prediction: 62; True Label: 62\n",
      "Index: 374; Prediction: 40; True Label: 40\n",
      "Index: 375; Prediction: 70; True Label: 70\n",
      "Index: 376; Prediction: 81; True Label: 81\n",
      "Index: 377; Prediction: 62; True Label: 62\n",
      "Index: 378; Prediction: 35; True Label: 2\n",
      "Index: 379; Prediction: 10; True Label: 10\n",
      "Index: 380; Prediction: 58; True Label: 43\n",
      "Index: 381; Prediction: 68; True Label: 68\n",
      "Index: 382; Prediction: 76; True Label: 76\n",
      "Index: 383; Prediction: 88; True Label: 88\n",
      "Index: 384; Prediction: 51; True Label: 51\n",
      "Index: 385; Prediction: 5; True Label: 54\n",
      "Index: 386; Prediction: 2; True Label: 2\n",
      "Index: 387; Prediction: 6; True Label: 6\n",
      "Index: 388; Prediction: 9; True Label: 9\n",
      "Index: 389; Prediction: 86; True Label: 86\n",
      "Index: 390; Prediction: 13; True Label: 13\n",
      "Index: 391; Prediction: 88; True Label: 88\n",
      "Index: 392; Prediction: 20; True Label: 20\n",
      "Index: 393; Prediction: 88; True Label: 87\n",
      "Index: 394; Prediction: 49; True Label: 49\n",
      "Index: 395; Prediction: 77; True Label: 77\n",
      "Index: 396; Prediction: 22; True Label: 22\n",
      "Index: 397; Prediction: 6; True Label: 6\n",
      "Index: 398; Prediction: 48; True Label: 48\n",
      "Index: 399; Prediction: 25; True Label: 51\n",
      "Index: 400; Prediction: 60; True Label: 60\n",
      "Index: 401; Prediction: 88; True Label: 88\n",
      "Index: 402; Prediction: 50; True Label: 24\n",
      "Index: 403; Prediction: 79; True Label: 79\n",
      "Index: 404; Prediction: 12; True Label: 38\n",
      "Index: 405; Prediction: 64; True Label: 64\n",
      "Index: 406; Prediction: 10; True Label: 10\n",
      "Index: 407; Prediction: 24; True Label: 0\n",
      "Index: 408; Prediction: 65; True Label: 65\n",
      "Index: 409; Prediction: 79; True Label: 79\n",
      "Index: 410; Prediction: 15; True Label: 31\n",
      "Index: 411; Prediction: 9; True Label: 16\n",
      "Index: 412; Prediction: 56; True Label: 30\n",
      "Index: 413; Prediction: 27; True Label: 65\n",
      "Index: 414; Prediction: 73; True Label: 73\n",
      "Index: 415; Prediction: 20; True Label: 20\n",
      "Index: 416; Prediction: 4; True Label: 4\n",
      "Index: 417; Prediction: 13; True Label: 13\n",
      "Index: 418; Prediction: 14; True Label: 14\n",
      "Index: 419; Prediction: 0; True Label: 0\n",
      "Index: 420; Prediction: 76; True Label: 67\n",
      "Index: 421; Prediction: 70; True Label: 69\n",
      "Index: 422; Prediction: 21; True Label: 21\n",
      "Index: 423; Prediction: 24; True Label: 24\n",
      "Index: 424; Prediction: 54; True Label: 54\n",
      "Index: 425; Prediction: 9; True Label: 9\n",
      "Index: 426; Prediction: 87; True Label: 86\n",
      "Index: 427; Prediction: 31; True Label: 31\n",
      "Index: 428; Prediction: 10; True Label: 10\n",
      "Index: 429; Prediction: 18; True Label: 18\n",
      "Index: 430; Prediction: 39; True Label: 39\n",
      "Index: 431; Prediction: 69; True Label: 78\n",
      "Index: 432; Prediction: 10; True Label: 26\n",
      "Index: 433; Prediction: 65; True Label: 65\n",
      "Index: 434; Prediction: 49; True Label: 49\n",
      "Index: 435; Prediction: 54; True Label: 9\n",
      "Index: 436; Prediction: 24; True Label: 24\n",
      "Index: 437; Prediction: 45; True Label: 19\n",
      "Index: 438; Prediction: 52; True Label: 52\n",
      "Index: 439; Prediction: 52; True Label: 10\n",
      "Index: 440; Prediction: 9; True Label: 28\n",
      "Index: 441; Prediction: 10; True Label: 10\n",
      "Index: 442; Prediction: 71; True Label: 71\n",
      "Index: 443; Prediction: 49; True Label: 49\n",
      "Index: 444; Prediction: 48; True Label: 48\n",
      "Index: 445; Prediction: 23; True Label: 23\n",
      "Index: 446; Prediction: 69; True Label: 79\n",
      "Index: 447; Prediction: 28; True Label: 28\n",
      "Index: 448; Prediction: 8; True Label: 8\n",
      "Index: 449; Prediction: 55; True Label: 55\n",
      "Index: 450; Prediction: 54; True Label: 54\n",
      "Index: 451; Prediction: 33; True Label: 33\n",
      "Index: 452; Prediction: 40; True Label: 40\n",
      "Index: 453; Prediction: 7; True Label: 7\n",
      "Index: 454; Prediction: 51; True Label: 25\n",
      "Index: 455; Prediction: 57; True Label: 57\n",
      "Index: 456; Prediction: 36; True Label: 36\n",
      "Index: 457; Prediction: 72; True Label: 72\n",
      "Index: 458; Prediction: 35; True Label: 35\n",
      "Index: 459; Prediction: 65; True Label: 65\n",
      "Index: 460; Prediction: 31; True Label: 31\n",
      "Index: 461; Prediction: 70; True Label: 70\n",
      "Index: 462; Prediction: 64; True Label: 64\n",
      "Index: 463; Prediction: 63; True Label: 2\n",
      "Index: 464; Prediction: 60; True Label: 60\n",
      "Index: 465; Prediction: 59; True Label: 59\n",
      "Index: 466; Prediction: 72; True Label: 63\n",
      "Index: 467; Prediction: 53; True Label: 53\n",
      "Index: 468; Prediction: 66; True Label: 66\n",
      "Index: 469; Prediction: 84; True Label: 84\n",
      "Index: 470; Prediction: 73; True Label: 73\n",
      "Index: 471; Prediction: 54; True Label: 54\n",
      "Index: 472; Prediction: 14; True Label: 14\n",
      "Index: 473; Prediction: 51; True Label: 25\n",
      "Index: 474; Prediction: 18; True Label: 18\n",
      "Index: 475; Prediction: 83; True Label: 83\n",
      "Index: 476; Prediction: 67; True Label: 67\n",
      "Index: 477; Prediction: 8; True Label: 8\n",
      "Index: 478; Prediction: 22; True Label: 22\n",
      "Index: 479; Prediction: 21; True Label: 21\n",
      "Index: 480; Prediction: 69; True Label: 78\n",
      "Index: 481; Prediction: 57; True Label: 57\n",
      "Index: 482; Prediction: 57; True Label: 57\n",
      "Index: 483; Prediction: 56; True Label: 12\n",
      "Index: 484; Prediction: 77; True Label: 77\n",
      "Index: 485; Prediction: 64; True Label: 64\n",
      "Index: 486; Prediction: 36; True Label: 36\n",
      "Index: 487; Prediction: 62; True Label: 62\n",
      "Index: 488; Prediction: 68; True Label: 68\n",
      "Index: 489; Prediction: 27; True Label: 44\n",
      "Index: 490; Prediction: 66; True Label: 66\n",
      "Index: 491; Prediction: 9; True Label: 9\n",
      "Index: 492; Prediction: 62; True Label: 62\n",
      "Index: 493; Prediction: 22; True Label: 22\n",
      "Index: 494; Prediction: 43; True Label: 43\n",
      "Index: 495; Prediction: 74; True Label: 74\n",
      "Index: 496; Prediction: 15; True Label: 15\n",
      "Index: 497; Prediction: 73; True Label: 73\n",
      "Index: 498; Prediction: 18; True Label: 83\n",
      "Index: 499; Prediction: 8; True Label: 8\n",
      "Accuracy on valid set: 77.4%\n"
     ]
    }
   ],
   "source": [
    "all_chars = 500\n",
    "true_labels = 0\n",
    "for i in range(all_chars):\n",
    "    print(f\"Index: {i}; Prediction: {predicted_class_indices[i]}; True Label: {Y_val[i]}\")\n",
    "    if predicted_class_indices[i] == Y_val[i]:\n",
    "        true_labels += 1\n",
    "print(f\"Accuracy on valid set: {(true_labels/all_chars) * 100}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3638fd39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
